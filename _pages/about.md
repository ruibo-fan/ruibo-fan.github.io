---
layout: home
title: "Ruibo Fan"
subtitle: "Ph.D. Candidate in Data Science and Analytics, HKUST(GZ)"
permalink: /
author_profile: true
---

I am a Ph.D. candidate at the Hong Kong University of Science and Technology (Guangzhou), in the **Data Science and Analytics Thrust**. I am advised by **Prof. Xiaowen Chu** (primary advisor) and **Prof. Wei Wang** (co-advisor, HKUST CSE Department).

My research lies at the intersection of **high‑performance computing**, **GPU architecture analysis and optimization**, and **efficient AI systems**. I design algorithms and systems for:

- **Sparse matrix computations** and **graph neural network (GNN) acceleration**
- **Large Language Model (LLM) inference optimization**
- **GPU microarchitecture characterization and benchmarking**

My long‑term goal is to bridge the gap between **theoretical computer architecture** and **practical, high‑performance implementations** for real‑world workloads in AI, scientific computing, and large‑scale data processing.

---

## Research Interests

- GPU Architecture & Microarchitecture
- High‑Performance Computing (HPC)
- Parallel and Distributed Computing
- Sparse Matrix Operations & Sparse Tensor Computations
- Graph Neural Networks Acceleration
- Large Language Model Inference Systems
- Deep Learning Systems & Compilers

---

## News

- **Oct. 2025** – Started as a **Research Intern** at the Technology Risk and Efficiency (TRE) team, **Alibaba Group**, working on performance optimization and reliability analysis for large‑scale AI and data systems.
- **Sept. 2025** – Journal extension **“Exploiting Low‑Level Sparsity for Efficient Large Language Model Inference on GPUs with SpInfer”** invited to **ACM Transactions on Computer Systems (TOCS)** (CCF‑A, under review).
- **Aug. 2025** – Our work **“ROME: Maximizing GPU Efficiency for All‑Pairs Shortest Path via Taming Fine‑Grained Irregularities”** was accepted to **PPoPP 2026** (CCF‑A).
- **May 2025** – Our work **“ZipServ: Fast and Memory‑Efficient LLM Inference with Hardware‑Aware Lossless Compression”** was accepted to **ASPLOS 2026** (CCF‑A).
- **Apr. 2025** – Our paper **“SpInfer: Leveraging Low‑Level Sparsity for Efficient Large Language Model Inference on GPUs”** received the **Best Paper Award** at **EuroSys 2025**.
- **Feb. 2025** – **SpInfer** successfully passed artifact evaluation at EuroSys 2025 and received **three badges**: *Available*, *Functional*, and *Reproducible*.
- **Jan. 2025** – **SpInfer** was accepted to **EuroSys 2025** (30 papers accepted out of 367 new submissions in the Fall round).
- **Jan. 2025** – Our collaborative work **“STBLLM: Breaking the 1‑Bit Barrier with Structured Binary LLMs”** was accepted to **ICLR 2025**.
- **Jan. 2025** – Our preprint **“Dissecting the NVIDIA Hopper Architecture through Micro‑benchmarking and Multiple Level Analysis”** was released on **arXiv** (journal version under review at TOCS).

[View all news](/news/)
