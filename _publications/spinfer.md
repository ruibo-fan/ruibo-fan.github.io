---
title: "SpInfer: Leveraging Low-Level Sparsity for Efficient Large Language Model Inference on GPUs"
collection: publications
venue: "Proceedings of the Twentieth European Conference on Computer Systems (EuroSys)"
date: 2025-04-01
order: 3
citation: "**Ruibo Fan**, et al., \"SpInfer: Leveraging Low-Level Sparsity for Efficient Large Language Model Inference on GPUs,\" in *Proceedings of the 20th European Conference on Computer Systems (EuroSys)*, 2025."

paperurl: "https://dl.acm.org/doi/abs/10.1145/3689031.3717481"
codeurl:  "https://github.com/xxyux/SpInfer/"
bibtexurl: "https://your-spinfer-eurosys-bibtex-url"
type: "conferences"
category: conferences
---

**SpInfer** exploits **low-level sparsity** in LLM computation on GPUs to accelerate inference. By co-designing sparse kernels, data layouts, and runtime scheduling, SpInfer reduces unnecessary computation and memory traffic while preserving model accuracy, achieving significantly lower latency and higher throughput compared to dense baselines.
