---
title: "ZipServ: Fast and Memory-Efficient LLM Inference with Hardware-Aware Lossless Compression"
collection: publications
venue: "Proceedings of the 31st ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)"
date: 2026-03-01
order: 1

citation: "**Ruibo Fan**, Xiangrui Yu, Xinglin Pan, Zeyu Li, Weile Luo, Qiang Wang, Wei Wang, and Xiaowen Chu, ‘‘ZipServ: Fast and Memory-Efficient LLM Inference with Hardware-Aware Lossless Compression,’’ in the Proceedings of ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’26), Pittsburgh, PA, USA, March 2026."

paperurl: "https://your-paper-url"
codeurl:  "https://your-code-url"
bibtexurl: "https://your-bibtex-url"
type: "conferences"
category: conferences
---

We propose **ZipServ**, a hardware‑aware lossless compression framework for **large language model (LLM) inference**. ZipServ co‑optimizes compression formats, memory layout, and GPU execution to reduce memory footprint and bandwidth pressure while preserving exact numerical correctness. Our evaluation on state‑of‑the‑art LLMs demonstrates significant speedups and memory savings over existing inference systems, without sacrificing output quality.
