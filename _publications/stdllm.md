---
title: "STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs"
collection: publications
venue: "The Thirteenth International Conference on Learning Representations (ICLR)"
date: 2025-05-01

citation: "Peng Dong, Lin Li, Yuke Zhong, Dazhen Du, **Ruibo Fan**, Yuxin Chen, et al., \"STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs,\" in *Proceedings of the 13th International Conference on Learning Representations (ICLR)*, 2025."

paperurl: "https://your-stbllm-iclr-paper-url"
codeurl:  "https://your-stbllm-iclr-code-url"
bibtexurl: "https://your-stbllm-iclr-bibtex-url"
type: "conferences"
category: conferences
---

**STBLLM** introduces **structured binary large language models** that push quantization beyond the **1-bit barrier**. By leveraging structured binary formats and tailored training techniques, STBLLM maintains competitive accuracy while dramatically reducing memory footprint and computation costs.
