---
title: "ZipServ: Fast and Memory-Efficient LLM Inference with Hardware-Aware Lossless Compression"
collection: publications
permalink: /publication/zipserv-asplos26
date: 2026-03-01
venue: "Proceedings of the 31st ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)"
venue_short: "ASPLOS 2026"
authors: "Ruibo Fan, et al."
citation: "R. Fan et al., \"ZipServ: Fast and Memory-Efficient LLM Inference with Hardware-Aware Lossless Compression,\" in <i>ASPLOS 2026</i>."
paperurl: "https://your-link-to-pdf-or-arxiv"
codeurl: "https://your-link-to-code-repo"
type: "conference"
category: "LLM Inference, Compression, HPC"
---

We propose **ZipServ**, a hardware‑aware lossless compression framework for **large language model (LLM) inference**. ZipServ co‑optimizes compression formats, memory layout, and GPU execution to reduce memory footprint and bandwidth pressure while preserving exact numerical correctness. Our evaluation on state‑of‑the‑art LLMs demonstrates significant speedups and memory savings over existing inference systems, without sacrificing output quality.
